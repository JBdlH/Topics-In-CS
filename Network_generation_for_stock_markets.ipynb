{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hogCRXC_OekR"
      },
      "source": [
        "##Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSbaExAFl1vB",
        "outputId": "e90dc7fd-faa0-4827-81da-dec24bf062ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stats\n",
            "  Downloading stats-0.1.2a.tar.gz (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.6/127.6 kB\u001b[0m \u001b[31m707.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: stats\n",
            "  Building wheel for stats (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stats: filename=stats-0.1.2a0-py3-none-any.whl size=24298 sha256=eec0eaf377327a810dc75b16797800aa3876833928036ee5d30eaf46ebba851f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/85/bc/3371b9bce1e4f7b8b638e8c968a4dbd74db171ee180c48f808\n",
            "Successfully built stats\n",
            "Installing collected packages: stats\n",
            "Successfully installed stats-0.1.2a0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for upgrade\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.10/dist-packages (0.16)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from python-louvain) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from python-louvain) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yahoofinancials\n",
            "  Downloading yahoofinancials-1.14.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from yahoofinancials) (2022.7.1)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.10/dist-packages (from yahoofinancials) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yahoofinancials) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yahoofinancials) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yahoofinancials) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yahoofinancials) (1.26.15)\n",
            "Building wheels for collected packages: yahoofinancials\n",
            "  Building wheel for yahoofinancials (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yahoofinancials: filename=yahoofinancials-1.14-py3-none-any.whl size=28643 sha256=23dd2b4888266b57162f55c26ac7f10b5b1254eb7e5b3cd2982ce8c0fc54fd04\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/a3/b6/b5b33187f2d42f095fecc236b957b46173fa09e78a106e309f\n",
            "Successfully built yahoofinancials\n",
            "Installing collected packages: yahoofinancials\n",
            "Successfully installed yahoofinancials-1.14\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.5.3.2-py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.2/358.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.22.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from tslearn) (0.56.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from tslearn) (1.2.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->tslearn) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->tslearn) (3.1.0)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.5.3.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.18)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.2)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.3.7)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: cryptography>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (40.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3.2->yfinance) (1.15.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3.2->yfinance) (2.21)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install stats\n",
        "!pip install upgrade networkx\n",
        "\n",
        "!pip install yahoofinancials\n",
        "!pip install tslearn\n",
        "!pip install yfinance\n",
        "import math\n",
        "import networkx.algorithms.community as nx_comm\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tslearn.metrics import dtw_path\n",
        "from tslearn.metrics import dtw\n",
        "from sklearn.preprocessing import normalize\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import plotly.graph_objs as go\n",
        "import random\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from scipy import stats\n",
        "import datetime\n",
        "import yfinance as yf\n",
        "from yahoofinancials import YahooFinancials\n",
        "import networkx as nx\n",
        "from itertools import combinations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByDyEC5AdbAd"
      },
      "source": [
        "##Set-up functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avCvFNEYeepn"
      },
      "outputs": [],
      "source": [
        "def distance_dist(M):\n",
        "    shape = M.shape\n",
        "    M = M.reshape(-1,1)\n",
        "    nonM = M[M != 0]\n",
        "    series = nonM.tolist()\n",
        "    data_sorted = np.sort(nonM)\n",
        "    N = len(M)\n",
        "    for i in range(len(data_sorted)):\n",
        "        if i/N > 0.975: \n",
        "            nonM[series.index(data_sorted[i])] = 1\n",
        "        elif i/N > 0.95:\n",
        "            nonM[series.index(data_sorted[i])] = 0.1\n",
        "        elif i/N > 0.9:\n",
        "            nonM[series.index(data_sorted[i])] = 0.01\n",
        "        else:\n",
        "            nonM[series.index(data_sorted[i])] = 0.001\n",
        "    M = np.zeros(shape)\n",
        "    M[np.triu_indices(shape[0], 1)] = nonM\n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMn6KL9FLxTn"
      },
      "outputs": [],
      "source": [
        "def get_data(term = 'short'):\n",
        "    tickers = pd.read_html(\n",
        "        'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
        "\n",
        "    # Get the data for this tickers from yahoo finance\n",
        "    if term == 'long':\n",
        "        data = yf.download(tickers.Symbol.to_list(), interval = \"1d\",  start = '2019-11-1', end = '2023-4-17')['Close']\n",
        "    elif term == 'mid':\n",
        "      data = yf.download(tickers.Symbol.to_list(), interval = \"1h\",  start = '2022-5-20', end = '2023-4-25')['Close']\n",
        "    else:\n",
        "      data = yf.download(tickers.Symbol.to_list(), interval = \"5m\",  start = '2023-3-1', end = '2023-4-30')['Close']\n",
        "      #data2 = yf.download(tickers.Symbol.to_list(), interval = \"5m\",  start = '2023-4-11', end = '2023-4-18')['Close']\n",
        "      #data3 = yf.download(tickers.Symbol.to_list(), interval = \"1m\",  start = '2023-4-3', end = '2023-4-9')['Close']\n",
        "     # data = pd.concat([data1,data2])\n",
        "    null_sum = data.isnull().sum(axis = 0)\n",
        "    for col in data.columns:\n",
        "        if null_sum[col]>(len(data[col])/20):\n",
        "            data = data.drop(labels = col, axis = 1)\n",
        "            print (col)\n",
        "        data = data.fillna(method='ffill')\n",
        "\n",
        "    df_std = data.copy(deep = True)\n",
        "    # apply the z-score method\n",
        "    for column in df_std.columns:\n",
        "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
        "        \n",
        "    return (df_std, data)\n",
        "#print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_capitalization():\n",
        "    tickers = pd.read_html(\n",
        "        'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
        "    FMP_key = '36f8d80afffeb9577beb82ec167451eb'\n",
        "    symbols = []\n",
        "    market_caps = []\n",
        "    for item in tickers:\n",
        "       \n",
        "        response = requests.get(url(symbol = item, api_key = FMP_key)).json()\n",
        "        symbols.append(response[0]['symbol'])\n",
        "        market_caps.append(response[0]['marketCap'])\n",
        "    cap = {}\n",
        "    for i in range(len(symbols)):\n",
        "        cap[symbols[i]]=market_caps[i]\n",
        "\n",
        "    \n",
        "    return cap\n",
        "\n",
        "def url(symbol: str, api_key):\n",
        "    return \"https://financialmodelingprep.com/api/v3/market-capitalization/\" + symbol + \"?apikey=\" + api_key\n"
      ],
      "metadata": {
        "id": "fQBOtm-MKTF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4UImk14SNWj"
      },
      "outputs": [],
      "source": [
        "def setMatrix(data):\n",
        "  mat = []\n",
        "  companies = data.columns\n",
        "  check_comp = data.columns.to_list()\n",
        "  for s1 in companies:\n",
        "      check_comp.remove(s1)\n",
        "      sim_vect = np.zeros(len(companies)-len(check_comp))\n",
        "      for s2 in check_comp:\n",
        "        sim_vect = np.append(sim_vect,(dtw(data[[s1]], data[[s2]])))\n",
        "      mat.append(sim_vect)\n",
        "  mat = np.array(mat)\n",
        "  shape = mat.shape\n",
        "  #mat.reshape(-1,1)\n",
        "  #mat = normalize(mat)\n",
        "  #mat.reshape(shape)\n",
        "  return mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdVHbs2ae5lv"
      },
      "outputs": [],
      "source": [
        "def dist(M):\n",
        "    # Get the upper triangle of the distance matrix\n",
        "    triu_indices = np.triu_indices_from(M, k=1)\n",
        "    distances = M[triu_indices].tolist()\n",
        "    # Calculate the percentiles of the distances\n",
        "    #percentiles = np.percentile(distances, np.arange(0, 100.5, 0.5))\n",
        "\n",
        "    # Convert the distances to percentiles\n",
        "    p_5 = np.percentile(distances, 5)\n",
        "    p_25 = np.percentile(distances, 25)\n",
        "    p_50 = np.percentile(distances, 50)\n",
        "    p_10 = np.percentile(distances, 10)\n",
        "    percentile_values = np.zeros_like(distances, dtype=float)\n",
        "    #distances_percentiles = [stats.percentileofscore(distances, d, 'mean') for d in distances]\n",
        "    #distances_percentiles = [ np.nanpercentile(distances, d) for d in distances]\n",
        "    i = 0\n",
        "    for d in distances:\n",
        "        if d <= p_5:\n",
        "            percentile_values[i] = 1\n",
        "        elif d <= p_10:\n",
        "            percentile_values[i] = 0.7\n",
        "        elif d <= p_25:\n",
        "            percentile_values[i] = 0.5\n",
        "        elif d <= p_50:\n",
        "            percentile_values[i] = 0.1\n",
        "        else:\n",
        "            percentile_values[i] = 0.05\n",
        "            #percentile_values[i] = 0.1*distances[i]\n",
        "        i += 1\n",
        "        #percentile_values[i] = val\n",
        "        \n",
        "    # Create a new matrix with the percentiles\n",
        "    dist_matrix = np.zeros_like(M, dtype=float)\n",
        "    dist_matrix[triu_indices] = percentile_values\n",
        "\n",
        "    # Copy the values to the lower triangle to make the matrix symmetric\n",
        "    #dist_matrix += dist_matrix.T\n",
        "\n",
        "    return dist_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp-K9tT8UATk"
      },
      "outputs": [],
      "source": [
        "def dist_uncomplete(M):\n",
        "    # Get the upper triangle of the distance matrix\n",
        "    triu_indices = np.triu_indices_from(M, k=1)\n",
        "    distances = M[triu_indices]\n",
        "\n",
        "    # Calculate the percentiles of the distances\n",
        "    #percentiles = np.percentile(distances, np.arange(0, 100.5, 0.5))\n",
        "\n",
        "    # Convert the distances to percentiles\n",
        "    percentile_values = np.zeros_like(distances, dtype=float)\n",
        "    p_5 = np.percentile(distances, 5)\n",
        "    p_25 = np.percentile(distances, 25)\n",
        "    p_50 = np.percentile(distances, 50)\n",
        "    p_10 = np.percentile(distances, 10)    \n",
        "    i = 0\n",
        "    for d in distances:\n",
        "        if d <= p_5:\n",
        "            percentile_values[i] = 1\n",
        "        elif d <= p_10:\n",
        "            percentile_values[i] = 0.5\n",
        "        elif d <= p_25:\n",
        "            percentile_values[i] = 0.1\n",
        "        elif d <= p_50:\n",
        "            percentile_values[i] = 0\n",
        "        else:\n",
        "            percentile_values[i] = 0\n",
        "            #percentile_values[i] = 0.1*distances[i]\n",
        "        i += 1\n",
        "        #percentile_values[i] = val\n",
        "        \n",
        "    # Create a new matrix with the percentiles\n",
        "    dist_matrix = np.zeros_like(M, dtype=float)\n",
        "    dist_matrix[triu_indices] = percentile_values\n",
        "\n",
        "    # Copy the values to the lower triangle to make the matrix symmetric\n",
        "    #dist_matrix += dist_matrix.T\n",
        "\n",
        "    return dist_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oq8bDsn694n"
      },
      "outputs": [],
      "source": [
        "def set_graph(M, l, data):\n",
        "    G = nx.from_numpy_array(M, create_using=nx.Graph)\n",
        "    d = {i:l[i] for i in G.nodes()}\n",
        "    G = nx.relabel_nodes(G, d)\n",
        "    data = data.tail(2)\n",
        "    last_price = {node:data[node][-1] for node in G.nodes()}\n",
        "    variation = {node:(data[node][-1]-data[node][0])/(data[node][0]) for node in G.nodes()}\n",
        "    nx.set_node_attributes(G, last_price, name='value')\n",
        "    nx.set_node_attributes(G, variation, name='variation')\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzhGrAR2EllY"
      },
      "outputs": [],
      "source": [
        "def comm_var(G, comm, measure):\n",
        "    #var = {i:0 for i in range(len(comm))}\n",
        "    var =[]\n",
        "    i = 0\n",
        "    for c in comm:\n",
        "        count = 0\n",
        "        for company in c:\n",
        "            count += measure[company]\n",
        "        var.append(count/len(c))\n",
        "        i+=1\n",
        "    return var\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8hAERLoTwUr"
      },
      "outputs": [],
      "source": [
        "def weighted_moving_average(data, length):\n",
        "    data = data.iloc[:length]\n",
        "    vars = {}\n",
        "    val = 0\n",
        "    for company in data.columns:\n",
        "        av = 0\n",
        "        for i in range(len(data[company])-1):\n",
        "            av += (2*i/(length*(length+1)))*(data[company][i+1]-data[company][i])/data[company][i]\n",
        "            t= val\n",
        "        vars[company]= av\n",
        "    return vars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je-w1SuFNkY-"
      },
      "outputs": [],
      "source": [
        "def moving_av(data, length):\n",
        "    data = data.iloc[:length]\n",
        "    av = 0\n",
        "    vars = {}\n",
        "    for company in data.columns:\n",
        "        for i in range(len(data[company])-1):\n",
        "            av = av + (data[company][i+1]-data[company][i])/data[company][i]\n",
        "        av = av/length\n",
        "        vars[company]= av\n",
        "    return vars"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def log_return(data, m=5):\n",
        "    l = {}\n",
        "    for company in data.columns:\n",
        "        l[company] = np.log(data[company][5]/data[company][0])\n",
        "    return l"
      ],
      "metadata": {
        "id": "R0ovGUIS-MID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2fRUR8WG69j"
      },
      "outputs": [],
      "source": [
        "def update_node(G, node, comm, measure):\n",
        "    i = 0\n",
        "    for c in comm:\n",
        "        if node in c:\n",
        "          index = i\n",
        "          break\n",
        "        i+=1\n",
        "    val = 0\n",
        "    w = 0\n",
        "    for company in comm[index]:\n",
        "        if company != node:\n",
        "            val = val+measure[company]*G.get_edge_data(node, company)['weight']\n",
        "            w = w + G.get_edge_data(node, company)['weight']\n",
        "    val = val/w\n",
        "    return val "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-B_8UxWDnLV"
      },
      "outputs": [],
      "source": [
        "def update_node_mov(G, node, comm, measure):\n",
        "    i = 0\n",
        "    for c in comm:\n",
        "        if node in c:\n",
        "          index = i\n",
        "          break\n",
        "        i+=1\n",
        "    val = 0\n",
        "    w = []\n",
        "    values = []\n",
        "    for company in comm[index]:\n",
        "        w.append(G.get_edge_data(node, company, default = {'weight':0})['weight'])\n",
        "        values.append(measure[company])\n",
        "    return weighted_average(values, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvPMy3PYPv_o"
      },
      "outputs": [],
      "source": [
        "def weighted_average(distribution, weights):\n",
        "    return sum([distribution[i]*weights[i] for i in range(len(distribution))])/sum(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rry_JW6G1LpQ"
      },
      "outputs": [],
      "source": [
        "def detect_n_comm_outlier(G,measure, communities,n):\n",
        "    buy = {}\n",
        "    short = {}\n",
        "    comp_out = ''\n",
        "    average_measure = comm_var(G, communities, measure)\n",
        "    multiplier = [v/max(average_measure) for v in average_measure]\n",
        "    avg = sum(measure.values())/len(measure)\n",
        "    j=0\n",
        "    vals = {}\n",
        "    for comm in communities:\n",
        "        #if variation_comm[i]<0 and variation_comm[i]<-1.0*val:\n",
        "        if average_measure[j]<0:\n",
        "            vals =  {company:(measure[company]-update_node(G, company, communities, measure)) for company in comm}\n",
        "            c = sorted(vals.items(), key=lambda item: item[1])\n",
        "            for i in range(int(len(comm)*n)):\n",
        "                #if c[i][1] > 0:\n",
        "                short[c[i][0]] = abs(multiplier[j])\n",
        "        #elif variation_comm[i]>val and variation_comm[i]>0:\n",
        "        else:\n",
        "            vals =  {company:(update_node(G, company, communities, measure)-measure[company]) for company in comm}\n",
        "            c = sorted(vals.items(), key=lambda item: item[1])\n",
        "            for i in range(int(len(comm)*n)):\n",
        "                #if c[i][1] > 0:\n",
        "                buy[c[i][0]] = abs(multiplier[j])\n",
        "        j = j+1\n",
        "    return buy, short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyUY-CK_MhCc"
      },
      "outputs": [],
      "source": [
        "def detect_comm_outlier(G,val,measure, communities):\n",
        "    buy = {}\n",
        "    short = {}\n",
        "    comp_out = ''\n",
        "    average_measure = comm_var(G, communities, measure)\n",
        "    i=0\n",
        "    comm_vals = {}\n",
        "    for comm in communities:\n",
        "        max = 0\n",
        "        #if variation_comm[i]<0 and variation_comm[i]<-1.0*val:\n",
        "        if average_measure[i]<0:\n",
        "            for company in comm:\n",
        "                val = measure[company]-update_node(G, company, communities, measure)\n",
        "                if val>abs(avg):\n",
        "                    comp_out = company\n",
        "                    max = val\n",
        "            if max !=0:\n",
        "                short[comp_out] = max\n",
        "        #elif variation_comm[i]>val and variation_comm[i]>0:\n",
        "        else:\n",
        "            for company in comm:\n",
        "                val = update_node(G, company, communities, measure)-measure[company]\n",
        "                if val>abs(avg):\n",
        "                    comp_out = company\n",
        "                    max = val\n",
        "            if max !=0:\n",
        "                buy[comp_out] = max\n",
        "        i +=1\n",
        "    return buy, short\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YejoOOZo0qdi"
      },
      "outputs": [],
      "source": [
        "def detect_comm_outlier_mod(G,measure, communities):\n",
        "    average_measure = comm_var(G, communities, measure)\n",
        "    i=0\n",
        "    buy = {}\n",
        "    short = {}\n",
        "    avg = sum(measure.values())/len(measure)\n",
        "    for comm in communities:\n",
        "        if avg>average_measure[i]: \n",
        "             for company in comm:\n",
        "                val = measure[company]-update_node(G, company, communities, measure)\n",
        "                if val >0:\n",
        "                    short[company]= val\n",
        "        if average_measure[i]>avg: \n",
        "             for company in comm:\n",
        "                val = update_node(G, company, communities, measure)-measure[company]\n",
        "                if val >0:\n",
        "                    buy[company]= val\n",
        "        i=i+1\n",
        "    return buy, short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG_Moe0PDUEG"
      },
      "outputs": [],
      "source": [
        "def community_analysis_cont(G,variation, communities):\n",
        "    strat = {}\n",
        "    comp_out = ''\n",
        "    i=0\n",
        "    short = {}\n",
        "    buy = {}\n",
        "    for comm in communities:\n",
        "        max = 0\n",
        "        for company in comm:\n",
        "          val = variation[company]-update_node(G, company, communities, variation)\n",
        "          if val>=0:\n",
        "              short[company] = val\n",
        "          else:\n",
        "              buy[company] = abs(val)\n",
        "    return short, buy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnFpMzKylREo"
      },
      "outputs": [],
      "source": [
        "def gr_un(data, length):\n",
        "    companies = data.columns.tolist()\n",
        "    data = data.iloc[:length]\n",
        "    matrix = setMatrix(data)\n",
        "    dist_matrix = dist_uncomplete(matrix)\n",
        "    G = set_graph(dist_matrix, companies, data)\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdIIUVT63AmT"
      },
      "outputs": [],
      "source": [
        "def gr(data, length):\n",
        "    companies = data.columns.tolist()\n",
        "    data = data.iloc[:length]\n",
        "    matrix = setMatrix(data)\n",
        "    dist_matrix = dist(matrix)\n",
        "    G = set_graph(dist_matrix, companies, data)\n",
        "    return G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1kR_Fx_Q_96"
      },
      "source": [
        "##Community Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiOigJzTLVKs"
      },
      "outputs": [],
      "source": [
        "def compute_perm_adj(G, comm):\n",
        "    G_perm = rearrange_graph(G, comm)\n",
        "    A_perm = nx.adjacency_matrix(G_perm).todense()\n",
        "    \n",
        "    return A_perm\n",
        "\n",
        "def rearrange_graph(G, comm):\n",
        "    G_perm = G.copy(); G_perm.clear() \n",
        "    \n",
        "    comm_copy = comm.copy(); \n",
        "    comm_copy.sort(key=len, reverse = True) \n",
        "    \n",
        "    for comm_set in comm_copy:\n",
        "        G_perm.add_nodes_from(list(comm_set))\n",
        "    \n",
        "    for edge in G.edges.data():\n",
        "        G_perm.add_edges_from([edge])\n",
        "        \n",
        "    return G_perm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi5n52jHQ_1V"
      },
      "outputs": [],
      "source": [
        "def compute_asso_one_run(G, perm_comm, seed):\n",
        "\n",
        "    G_c = nx_comm.louvain_communities(G, seed = seed) \n",
        "    #G_c = nx_comm.asyn_lpa_communities(G, weight='weight')\n",
        "    G_comm = []\n",
        "    for comm in G_c:\n",
        "        G_comm.append(set(comm))\n",
        "    # get a graph with permuted nodes but no edges\n",
        "    G_dummy = rearrange_graph(G, perm_comm); G_dummy.remove_edges_from(list(G_dummy.edges))\n",
        "    \n",
        "    for comm in G_comm:\n",
        "        comm = list(comm)\n",
        "        G_dummy_edges = [(a, b) for idx, a in enumerate(comm) for b in comm[idx + 0:]]\n",
        "        G_dummy.add_edges_from(G_dummy_edges)\n",
        "    A_dummy = nx.adjacency_matrix(G_dummy).todense()\n",
        "    \n",
        "    return A_dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X__7OqK0RIzV"
      },
      "outputs": [],
      "source": [
        "def compute_asso(G, comm, n_runs, perm = True):\n",
        "    A_asso = np.zeros((len(G.nodes), len(G.nodes)))\n",
        "    \n",
        "    if perm:\n",
        "        for i in range(n_runs):\n",
        "            A_asso += compute_asso_one_run(G, comm, i)\n",
        "    else:\n",
        "        for i in range(n_runs):\n",
        "            A_asso += compute_asso_one_run(G, [set(G.nodes)], i)\n",
        "    A_asso /= n_runs\n",
        "    \n",
        "    return A_asso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpZNhSGSlKqU"
      },
      "outputs": [],
      "source": [
        "def compute_perm_adj(G, comm):\n",
        "    G_perm = rearrange_graph(G, comm)\n",
        "    A_perm = nx.adjacency_matrix(G_perm).todense()\n",
        "    \n",
        "    return A_perm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTEcW1Z_BB7l"
      },
      "source": [
        "##Non Temporal Strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKVigODj_w9y"
      },
      "outputs": [],
      "source": [
        "def strategy_moving_av(data, df, capital,m, iter, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = df.tail(len (data) - 300)\n",
        "    revenue = {} \n",
        "    moving_av = moving_average(data.iloc[300-m:], m)\n",
        "    variation = {node:(values[node][1]-values[node][0])/(values[node][0]) for node in companies}\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        buy, short = detect_comm_outlier_mod(G, 50 ,moving_av, communities)\n",
        "        moving_av = moving_average(data.iloc[300-m+i:], m)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i+1]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        buy_vals = [1+value for value in buy.values()]\n",
        "        var_buy = [variation[comp] for comp in buy.keys()]\n",
        "        short_vals = [1+value for value in short.values()]\n",
        "        var_short = [-1.0*variation[comp] for comp in short.keys()]\n",
        "        var = var_buy+var_short\n",
        "        vals = buy_vals+short_vals\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        our_avg = 0\n",
        "        if len (var)>0:\n",
        "            our_avg = (weighted_average(var, vals))\n",
        "            capital = (our_avg+1)*capital\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLQmT87WMhSd"
      },
      "outputs": [],
      "source": [
        "def strategy_outliers_var(data, df, capital,iter, G, n):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values =data.iloc[300:]\n",
        "    revenue = {} \n",
        "    variation = {node:(data[node][300]-values[node][299])/(values[node][299]) for node in companies}\n",
        "    total_revenue = 0\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        buy, short = detect_n_comm_outlier(G,variation, communities, 3)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i+1]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "        our_avg = our_avg/(len(short)+len(buy))\n",
        "        day_avg = sum(variation.values())/len(variation)\n",
        "        capital = capital*(1+our_avg) \n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KeYCeD3Hhvu"
      },
      "outputs": [],
      "source": [
        "def strategy_outliers_w(data, df, capital,iter,m, G, n):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[300:]\n",
        "    revenue = {} \n",
        "    measure = weighted_moving_average(data.iloc[300-m-1:], m)\n",
        "    total_revenue = 0\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        buy, short = detect_n_comm_outlier(G,measure, communities, 3)\n",
        "        measure =  weighted_moving_average(data.iloc[300-m+i:], m)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "        our_avg = our_avg/(len(short)+len(buy))\n",
        "        day_avg = sum(variation.values())/len(variation)\n",
        "        capital = capital*(1+our_avg) \n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmsYti3KC32w"
      },
      "outputs": [],
      "source": [
        "def strategy_mult_var(data, df, capital,iter, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[300:]\n",
        "    variation = {node:(data[node][300]-values[node][299])/(values[node][299]) for node in companies}    \n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        buy, short = detect_comm_outlier_mod(G, variation, communities)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "        our_avg= our_avg/(len(short)+len(buy))\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        capital = capital*(1.0+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4m9XzoOoaZK"
      },
      "outputs": [],
      "source": [
        "def strategy_mult_mov(data, df, capital,iter,m, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[300:]\n",
        "    measure = moving_av(data.iloc[300-m-1:], m)\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        buy, short = detect_comm_outlier_mod(G, measure, communities)\n",
        "        measure = moving_av(data.iloc[300-m+i:], m)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "        our_avg= our_avg/(len(short)+len(buy))\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        capital = capital*(1.0+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iBYqe7xobHO"
      },
      "outputs": [],
      "source": [
        "def strategy_mult_w(data, df, capital,iter,m, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[300:]\n",
        "    measure = log_return(data.iloc[300-5:], 5)\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        buy, short = detect_comm_outlier_mod(G, measure, communities)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        measure = log_return(data.iloc[300-5+i:], m)\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "        day_avg = sum(variation.values())/len(variation)\n",
        "        our_avg= our_avg/(len(short)+len(buy))\n",
        "        capital = capital*(1.0+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kTnR-bZyHfWw"
      },
      "outputs": [],
      "source": [
        "def strategy_outliers_mov(data, capital,iter,m, G, n):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[300:]\n",
        "    revenue = {} \n",
        "    measure = moving_av(data.iloc[300-m-1:], m)\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        s = 0\n",
        "        b = 0\n",
        "        our_avg = 0\n",
        "        buy, short = detect_n_comm_outlier(G,measure, communities,n)\n",
        "        measure = moving_av(data.iloc[300-m+i:], m)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        day_avg = sum(variation.values())/len(variation)\n",
        "        for company in buy.keys():\n",
        "            our_avg += variation[company]\n",
        "            b+=variation[company]\n",
        "        for company in short.keys():\n",
        "            our_avg = our_avg - variation[company]\n",
        "            s = s-variation[company]\n",
        "        if our_avg != 0:\n",
        "            our_avg = our_avg/(len(short)+len(buy))\n",
        "        capital = capital*(1+our_avg)\n",
        "        if len(short)>0: \n",
        "            s=s/len(short)\n",
        "        if len(buy)>0:\n",
        "            b=b/len(buy)\n",
        "        revenue.append([capital, day_avg, our_avg,b,s])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG7p8t-DsIHa"
      },
      "outputs": [],
      "source": [
        "def strategy_comp_log(data, df, capital,iter,m, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[500:]\n",
        "    measure = log_return(data.iloc[499:])\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        distribution = []\n",
        "        weights = []\n",
        "        buy, short = community_analysis_cont(G, measure, communities)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        measure = log_return(data.iloc[500+i:])\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in companies:\n",
        "            if company in buy.keys():\n",
        "                distribution.append(variation[company])\n",
        "                weights.append(buy[company])\n",
        "            else:\n",
        "                distribution.append(-1.0*variation[company])\n",
        "                weights.append(short[company])\n",
        "        w = sum(weights)\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] = weights[i]/w\n",
        "        our_avg = weighted_average(distribution, weights)\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        capital = capital*(1+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHUV63aEuD5u"
      },
      "outputs": [],
      "source": [
        "def strategy_comp_var(data, df, capital,iter, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[500:]\n",
        "    variation = {node:(data[node][500]-data[node][499])/(data[node][499]) for node in companies}    \n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        distribution = []\n",
        "        weights = []\n",
        "        buy, short = community_analysis_cont(G, variation, communities)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in companies:\n",
        "            if company in buy.keys():\n",
        "                distribution.append(variation[company])\n",
        "                weights.append(buy[company])\n",
        "            else:\n",
        "                distribution.append(-1.0*variation[company])\n",
        "                weights.append(short[company])\n",
        "        w = sum(weights)\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] = weights[i]/w\n",
        "        our_avg = weighted_average(distribution, weights)\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        capital = capital*(1+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYjXPxKXuEXh"
      },
      "outputs": [],
      "source": [
        "def strategy_comp_mov(data, df, capital,iter,m, G):\n",
        "    communities = nx_comm.louvain_communities(G)\n",
        "    companies = data.columns.tolist()\n",
        "    values = data.iloc[500:]\n",
        "    measure = moving_av(data.iloc[500-m:], m)\n",
        "    revenue = []\n",
        "    for i in range (iter):\n",
        "        our_avg = 0\n",
        "        distribution = []\n",
        "        weights = []\n",
        "        buy, short = community_analysis_cont(G, measure, communities)\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        measure = moving_av(data.iloc[500-m+i:], m)\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        for company in companies:\n",
        "            if company in buy.keys():\n",
        "                distribution.append(variation[company])\n",
        "                weights.append(buy[company])\n",
        "            else:\n",
        "                distribution.append(-1.0*variation[company])\n",
        "                weights.append(short[company])\n",
        "        w = sum(weights)\n",
        "        for i in range(len(weights)):\n",
        "            weights[i] = weights[i]/w\n",
        "        our_avg = weighted_average(distribution, weights)\n",
        "        day_avg = sum(variation.values()) / len(variation)\n",
        "        capital = capital*(1+our_avg)\n",
        "        revenue.append([capital, day_avg, our_avg])\n",
        "    return revenue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IovC1j1VA96S"
      },
      "source": [
        "##Temporal strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwAk7tKjcf51"
      },
      "outputs": [],
      "source": [
        "def temp_strat(temp_G, data, capital, iter,m,n):\n",
        "    communities_temp = []\n",
        "    companies = data.columns.tolist()\n",
        "    layers = len(temp_G)\n",
        "    values = data.iloc[layers*100:]\n",
        "    measure =  moving_av(data.iloc[layers*100-m-1:], m)\n",
        "    revenue = []\n",
        "    count = []\n",
        "    for G in temp_G:\n",
        "        comm = nx_comm.louvain_communities(G)\n",
        "        communities_temp.append(comm)\n",
        "    for i in range(iter):\n",
        "        s= 0\n",
        "        b = 0\n",
        "        buy_temp={comp:True for comp in companies}\n",
        "        short_temp={comp:True for comp in companies}\n",
        "        variation = {node:(values[node][i+1]-values[node][i])/(values[node][i]) for node in companies}\n",
        "        our_avg = 0\n",
        "        j=0\n",
        "        for G in temp_G:\n",
        "            buy, short = detect_n_comm_outlier(G, measure, communities_temp[j],n)\n",
        "            for company in companies:\n",
        "                if company not in buy.keys():\n",
        "                    buy_temp[company] = False\n",
        "                if company not in short.keys():\n",
        "                    short_temp[company] = False\n",
        "            j=j+1\n",
        "        measure =  moving_av(data.iloc[layers*100+i-m:], m)\n",
        "        counter = 0\n",
        "        for company in companies:\n",
        "            if buy_temp[company] == True:\n",
        "                our_avg = our_avg+variation[company]\n",
        "                b = b+variation[company]\n",
        "            if short_temp[company] == True:\n",
        "                our_avg = our_avg-variation[company]\n",
        "                s = s-variation[company]\n",
        "        if sum(buy_temp.values())>0:\n",
        "            b = b/sum(buy_temp.values())\n",
        "        if sum(short_temp.values())>0:\n",
        "            s = s/sum(short_temp.values())\n",
        "        if sum(buy_temp.values())+sum(short_temp.values())>0:\n",
        "            our_avg = our_avg/(sum(buy_temp.values())+sum(short_temp.values()))\n",
        "        day_avg = sum(variation.values())/len(variation)\n",
        "        capital = capital*(1+s)\n",
        "        revenue.append([capital,our_avg, day_avg,s,b])\n",
        "        count.append(counter)\n",
        "    return revenue, count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvbKT7fljz40"
      },
      "outputs": [],
      "source": [
        "def temporal_graph(data, layers,n, length = 100):\n",
        "    temp_G = []\n",
        "    end = n*layers*length\n",
        "    data = data.iloc[:end]\n",
        "    for i in range(layers):\n",
        "        data_layer=data.iloc[::-1*n*(i+1)]\n",
        "        G = gr(data_layer, length)\n",
        "        temp_G.append(G)\n",
        "    return temp_G"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def final_strat(data,df,layers,iter, capital, l=200, n):\n",
        "    re = iter/100\n",
        "    revenue = []\n",
        "    counter = []\n",
        "    for i in range(int(re)):\n",
        "        temp_G = temporal_graph(df.iloc[100*i:], layers,1,length = l)\n",
        "        r, c = temp_strat(temp_G, data.iloc[100*i:], 100, 100, 10, n)\n",
        "        revenue = revenue + r\n",
        "        counter = counter + c\n",
        "    return revenue, counter"
      ],
      "metadata": {
        "id": "13l3T2H_4ig5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}